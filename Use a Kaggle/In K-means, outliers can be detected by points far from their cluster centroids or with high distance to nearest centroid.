
در K-Means هر داده به نزدیک‌ترین مرکز خوشه نسبت داده می‌شود.
اگر فاصله‌ی یک نقطه از مرکز خوشه‌اش خیلی زیاد باشد → داده‌ی پرت.

معیار ریاضی:

برای هر نقطه :

d_i = \|x_i - \mu_{c(i)}\|_2

اگر:

d_i > \alpha \times \text{mean}(d)
\quad \text{یا} \quad
d_i > \mu_d + k\sigma_d

مزیت:

ساده و سریع


عیب:

به مقیاس داده حساس است



---

۲️⃣ استفاده از SSE یا Inertia

ایده:

K-Means تابع هدف زیر را مینیمم می‌کند:

J = \sum_{i=1}^{n} \|x_i - \mu_{c(i)}\|^2

نقاطی که سهم زیادی در SSE دارند، پرت‌اند.

معیار:

\|x_i - \mu_{c(i)}\|^2 \gg \text{میانگین}


---

۳️⃣ فاصله نسبت به شعاع خوشه (Cluster Radius)

ایده:

برای هر خوشه:

R_k = \max \|x - \mu_k\|

اگر نقطه‌ای:

\|x_i - \mu_k\| > \beta R_k


---

۴️⃣ خوشه‌های خیلی کوچک (Noise Cluster)

ایده:

اگر یک خوشه تعداد نمونه‌ی خیلی کمی داشته باشد:

|C_k| \ll n

آن خوشه احتمالاً شامل داده‌های پرت است.

کاربرد:

وقتی  بزرگ انتخاب شده باشد.


---

۵️⃣ مقایسه فاصله تا مرکز خوشه‌ی اول و دوم

ایده:

اگر یک نقطه تقریباً به دو خوشه به یک اندازه نزدیک باشد → غیرعادی است.

معیار:

\frac{d_1}{d_2} \approx 1

که  و  فاصله تا نزدیک‌ترین و دومین مرکز است.


---

۶️⃣ اجرای K-Means بعد از PCA

ایده:

ابتدا کاهش بُعد با PCA
بعد K-Means

داده‌های پرت در فضای PCA معمولاً دور از چگالی اصلی دیده می‌شوند.



---

۷️⃣ ترکیب با روش‌های دیگر (قوی‌تر)

در عمل:

K-Means برای Outlier Detection ایده‌آل نیست ❌

بهتر است با این روش‌ها ترکیب شود:


روش توضیح

DBSCAN خودش نویز را جدا می‌کند
Isolation Forest مبتنی بر درخت
LOF چگالی‌محور
One-Class SVM یادگیری ناحیه نرمال
